{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac19f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_id :  13151\n",
      "query :  SELECT YEAR, CREASE_OR_DECREASE_OVERALL_INDUSTRIAL_EXPORTS FROM IMPORT_EXPORT_STS_MOTIE_TEXTILE_I WHERE YEAR IN (SELECT YEAR FROM IMPORT_EXPORT_STS_MOTIE_TEXTILE_I WHERE TAL_INDUSTRIAL_INCOME_AMOUNT > 100000)\n",
      "type : 1-1-1 , level : extra\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### process_sql 필요함수\n",
    "import json\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "import jaydebeapi\n",
    "import re\n",
    "\n",
    "\n",
    "CLAUSE_KEYWORDS = ('select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', 'except')\n",
    "JOIN_KEYWORDS = ('join', 'on', 'as')\n",
    "\n",
    "WHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
    "UNIT_OPS = ('none', '-', '+', \"*\", '/')\n",
    "AGG_OPS = ('none', 'max', 'min', 'count', 'sum', 'avg')\n",
    "TABLE_TYPE = {\n",
    "    'sql': \"sql\",\n",
    "    'table_unit': \"table_unit\",\n",
    "}\n",
    "\n",
    "COND_OPS = ('and', 'or')\n",
    "SQL_OPS = ('intersect', 'union', 'except')\n",
    "ORDER_OPS = ('desc', 'asc')\n",
    "\n",
    "\n",
    "\n",
    "class Schema:\n",
    "    \"\"\"\n",
    "    Simple schema which maps table&column to a unique identifier\n",
    "    \"\"\"\n",
    "    def __init__(self, schema):\n",
    "        self._schema = schema\n",
    "        self._idMap = self._map(self._schema)\n",
    "\n",
    "    @property\n",
    "    def schema(self):\n",
    "        return self._schema\n",
    "\n",
    "    @property\n",
    "    def idMap(self):\n",
    "        return self._idMap\n",
    "\n",
    "    def _map(self, schema):\n",
    "        idMap = {'*': \"__all__\"}\n",
    "        id = 1\n",
    "        for key, vals in schema.items():\n",
    "            for val in vals:\n",
    "                idMap[key.lower() + \".\" + val.lower()] = \"__\" + key.lower() + \".\" + val.lower() + \"__\"\n",
    "                id += 1\n",
    "\n",
    "        for key in schema:\n",
    "            idMap[key.lower()] = \"__\" + key.lower() + \"__\"\n",
    "            id += 1\n",
    "\n",
    "        return idMap\n",
    "\n",
    "\n",
    "def get_schema(db):\n",
    "    \"\"\"\n",
    "    Get database's schema, which is a dict with table name as key\n",
    "    and list of column names as value\n",
    "    :param db: database path\n",
    "    :return: schema dict\n",
    "    \"\"\"\n",
    "\n",
    "    schema = {}\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # fetch table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [str(table[0].lower()) for table in cursor.fetchall()]\n",
    "\n",
    "    # fetch table info\n",
    "    for table in tables:\n",
    "        cursor.execute(\"PRAGMA table_info({})\".format(table))\n",
    "        schema[table] = [str(col[1].lower()) for col in cursor.fetchall()]\n",
    "\n",
    "    return schema\n",
    "\n",
    "\n",
    "def get_schema_from_json(fpath):\n",
    "    with open(fpath) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    schema = {}\n",
    "    for entry in data:\n",
    "        table = str(entry['table'].lower())\n",
    "        cols = [str(col['column_name'].lower()) for col in entry['col_data']]\n",
    "        schema[table] = cols\n",
    "\n",
    "    return schema\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    string = str(string)\n",
    "    string = string.replace(\"\\'\", \"\\\"\")  # ensures all string values wrapped by \"\" problem??\n",
    "    quote_idxs = [idx for idx, char in enumerate(string) if char == '\"']\n",
    "    assert len(quote_idxs) % 2 == 0, \"Unexpected quote\"\n",
    "\n",
    "    # keep string value as token\n",
    "    vals = {}\n",
    "    for i in range(len(quote_idxs)-1, -1, -2):\n",
    "        qidx1 = quote_idxs[i-1]\n",
    "        qidx2 = quote_idxs[i]\n",
    "        val = string[qidx1: qidx2+1]\n",
    "        key = \"__val_{}_{}__\".format(qidx1, qidx2)\n",
    "        string = string[:qidx1] + key + string[qidx2+1:]\n",
    "        vals[key] = val\n",
    "\n",
    "    toks = [word.lower() for word in word_tokenize(string)]\n",
    "    # replace with string value token\n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] in vals:\n",
    "            toks[i] = vals[toks[i]]\n",
    "\n",
    "    # find if there exists !=, >=, <=\n",
    "    eq_idxs = [idx for idx, tok in enumerate(toks) if tok == \"=\"]\n",
    "    eq_idxs.reverse()\n",
    "    prefix = ('!', '>', '<')\n",
    "    for eq_idx in eq_idxs:\n",
    "        pre_tok = toks[eq_idx-1]\n",
    "        if pre_tok in prefix:\n",
    "            toks = toks[:eq_idx-1] + [pre_tok + \"=\"] + toks[eq_idx+1: ]\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "def scan_alias(toks):\n",
    "    \"\"\"Scan the index of 'as' and build the map for all alias\"\"\"\n",
    "    as_idxs = [idx for idx, tok in enumerate(toks) if tok == 'as']\n",
    "    alias = {}\n",
    "    for idx in as_idxs:\n",
    "        alias[toks[idx+1]] = toks[idx-1]\n",
    "    return alias\n",
    "\n",
    "\n",
    "def get_tables_with_alias(schema, toks):\n",
    "    tables = scan_alias(toks) # as로 명명한 테이블들 없으면 공란 []\n",
    "    for key in schema:\n",
    "        assert key not in tables, \"Alias {} has the same name in table\".format(key)\n",
    "        tables[key] = key\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def parse_col(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    \"\"\"\n",
    "        :returns next idx, column id\n",
    "    \"\"\"\n",
    "    tok = toks[start_idx]\n",
    "    if tok == \"*\":\n",
    "        return start_idx + 1, schema.idMap[tok]\n",
    "\n",
    "    if '.' in tok:  # if token is a composite\n",
    "        alias, col = tok.split('.')\n",
    "        key = tables_with_alias[alias] + \".\" + col\n",
    "        return start_idx+1, schema.idMap[key]\n",
    "\n",
    "    assert default_tables is not None and len(default_tables) > 0, \"Default tables should not be None or empty\"\n",
    "\n",
    "    for alias in default_tables:\n",
    "        table = tables_with_alias[alias]\n",
    "        if tok in schema.schema[table]:\n",
    "            key = table + \".\" + tok\n",
    "            return start_idx+1, schema.idMap[key]\n",
    "\n",
    "    assert False, \"Error col: {}\".format(tok)\n",
    "\n",
    "\n",
    "def parse_col_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    \"\"\"\n",
    "        :returns next idx, (agg_op id, col_id)\n",
    "    \"\"\"\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    isBlock = False\n",
    "    isDistinct = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    if toks[idx] in AGG_OPS:\n",
    "        agg_id = AGG_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        assert idx < len_ and toks[idx] == '('\n",
    "        idx += 1\n",
    "        if toks[idx] == \"distinct\":\n",
    "            idx += 1\n",
    "            isDistinct = True\n",
    "        idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        assert idx < len_ and toks[idx] == ')'\n",
    "        idx += 1\n",
    "        return idx, (agg_id, col_id, isDistinct)\n",
    "\n",
    "    if toks[idx] == \"distinct\":\n",
    "        idx += 1\n",
    "        isDistinct = True\n",
    "    agg_id = AGG_OPS.index(\"none\")\n",
    "    idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n",
    "\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1  # skip ')'\n",
    "\n",
    "    return idx, (agg_id, col_id, isDistinct)\n",
    "\n",
    "\n",
    "def parse_val_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    isBlock = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    col_unit1 = None\n",
    "    col_unit2 = None\n",
    "    unit_op = UNIT_OPS.index('none')\n",
    "\n",
    "    idx, col_unit1 = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    if idx < len_ and toks[idx] in UNIT_OPS:\n",
    "        unit_op = UNIT_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        idx, col_unit2 = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1  # skip ')'\n",
    "\n",
    "    return idx, (unit_op, col_unit1, col_unit2)\n",
    "\n",
    "\n",
    "def parse_table_unit(toks, start_idx, tables_with_alias, schema):\n",
    "    \"\"\"\n",
    "        :returns next idx, table id, table name\n",
    "    \"\"\"\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    #print('toks', toks[idx])\n",
    "    #print('tables_with_alias', tables_with_alias)\n",
    "\n",
    "    key = tables_with_alias[toks[idx]]\n",
    "\n",
    "    if idx + 1 < len_ and toks[idx+1] == \"as\":\n",
    "        idx += 3\n",
    "    else:\n",
    "        idx += 1\n",
    "\n",
    "    return idx, schema.idMap[key], key\n",
    "\n",
    "\n",
    "def parse_value(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    isBlock = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    if toks[idx] == 'select':\n",
    "        idx, val = parse_sql(toks, idx, tables_with_alias, schema)\n",
    "    elif \"\\\"\" in toks[idx]:  # token is a string value\n",
    "        val = toks[idx]\n",
    "        idx += 1\n",
    "    else:\n",
    "        try:\n",
    "            val = float(toks[idx])\n",
    "            idx += 1\n",
    "        except:\n",
    "            end_idx = idx\n",
    "            while end_idx < len_ and toks[end_idx] != ',' and toks[end_idx] != ')'\\\n",
    "                and toks[end_idx] != 'and' and toks[end_idx] not in CLAUSE_KEYWORDS and toks[end_idx] not in JOIN_KEYWORDS:\n",
    "                    end_idx += 1\n",
    "\n",
    "            idx, val = parse_col_unit(toks[start_idx: end_idx], 0, tables_with_alias, schema, default_tables)\n",
    "            idx = end_idx\n",
    "\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1\n",
    "\n",
    "    return idx, val\n",
    "\n",
    "\n",
    "def parse_condition(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    conds = []\n",
    "\n",
    "    while idx < len_:\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        not_op = False\n",
    "        if toks[idx] == 'not':\n",
    "            not_op = True\n",
    "            idx += 1\n",
    "\n",
    "        assert idx < len_ and toks[idx] in WHERE_OPS, \"Error condition: idx: {}, tok: {}\".format(idx, toks[idx])\n",
    "        op_id = WHERE_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        val1 = val2 = None\n",
    "        if op_id == WHERE_OPS.index('between'):  # between..and... special case: dual values\n",
    "            idx, val1 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            assert toks[idx] == 'and'\n",
    "            idx += 1\n",
    "            idx, val2 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        else:  # normal case: single value\n",
    "            idx, val1 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            val2 = None\n",
    "\n",
    "        conds.append((not_op, op_id, val_unit, val1, val2))\n",
    "\n",
    "        if idx < len_ and (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\") or toks[idx] in JOIN_KEYWORDS):\n",
    "            break\n",
    "\n",
    "        if idx < len_ and toks[idx] in COND_OPS:\n",
    "            conds.append(toks[idx])\n",
    "            idx += 1  # skip and/or\n",
    "\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "def parse_select(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    assert toks[idx] == 'select', \"'select' not found\"\n",
    "    idx += 1\n",
    "    isDistinct = False\n",
    "    if idx < len_ and toks[idx] == 'distinct':\n",
    "        idx += 1\n",
    "        isDistinct = True\n",
    "    val_units = []\n",
    "\n",
    "    while idx < len_ and toks[idx] not in CLAUSE_KEYWORDS:\n",
    "        agg_id = AGG_OPS.index(\"none\")\n",
    "        if toks[idx] in AGG_OPS:\n",
    "            agg_id = AGG_OPS.index(toks[idx])\n",
    "            idx += 1\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        val_units.append((agg_id, val_unit))\n",
    "        if idx < len_ and toks[idx] == ',':\n",
    "            idx += 1  # skip ','\n",
    "\n",
    "    return idx, (isDistinct, val_units)\n",
    "\n",
    "\n",
    "def parse_from(toks, start_idx, tables_with_alias, schema):\n",
    "    \"\"\"\n",
    "    Assume in the from clause, all table units are combined with join\n",
    "    \"\"\"\n",
    "    assert 'from' in toks[start_idx:], \"'from' not found\"\n",
    "\n",
    "    len_ = len(toks)\n",
    "    idx = toks.index('from', start_idx) + 1\n",
    "    default_tables = []\n",
    "    table_units = []\n",
    "    conds = []\n",
    "\n",
    "    while idx < len_:\n",
    "        isBlock = False\n",
    "        if toks[idx] == '(':\n",
    "            isBlock = True\n",
    "            idx += 1\n",
    "\n",
    "        if toks[idx] == 'select':\n",
    "            idx, sql = parse_sql(toks, idx, tables_with_alias, schema)\n",
    "            table_units.append((TABLE_TYPE['sql'], sql))\n",
    "        else:\n",
    "            if idx < len_ and toks[idx] == 'join':\n",
    "                idx += 1  # skip join\n",
    "            idx, table_unit, table_name = parse_table_unit(toks, idx, tables_with_alias, schema)\n",
    "            table_units.append((TABLE_TYPE['table_unit'],table_unit))\n",
    "            default_tables.append(table_name)\n",
    "        if idx < len_ and toks[idx] == \"on\":\n",
    "            idx += 1  # skip on\n",
    "            idx, this_conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            if len(conds) > 0:\n",
    "                conds.append('and')\n",
    "            conds.extend(this_conds)\n",
    "\n",
    "        if isBlock:\n",
    "            assert toks[idx] == ')'\n",
    "            idx += 1\n",
    "        if idx < len_ and (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n",
    "            break\n",
    "\n",
    "    return idx, table_units, conds, default_tables\n",
    "\n",
    "\n",
    "def parse_where(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'where':\n",
    "        return idx, []\n",
    "\n",
    "    idx += 1\n",
    "    idx, conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "def parse_group_by(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    col_units = []\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'group':\n",
    "        return idx, col_units\n",
    "\n",
    "    idx += 1\n",
    "    assert toks[idx] == 'by'\n",
    "    idx += 1\n",
    "\n",
    "    while idx < len_ and not (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n",
    "        idx, col_unit = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        col_units.append(col_unit)\n",
    "        if idx < len_ and toks[idx] == ',':\n",
    "            idx += 1  # skip ','\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return idx, col_units\n",
    "\n",
    "\n",
    "def parse_order_by(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    val_units = []\n",
    "    order_type = 'asc' # default type is 'asc'\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'order':\n",
    "        return idx, val_units\n",
    "\n",
    "    idx += 1\n",
    "    assert toks[idx] == 'by'\n",
    "    idx += 1\n",
    "\n",
    "    while idx < len_ and not (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        val_units.append(val_unit)\n",
    "        if idx < len_ and toks[idx] in ORDER_OPS:\n",
    "            order_type = toks[idx]\n",
    "            idx += 1\n",
    "        if idx < len_ and toks[idx] == ',':\n",
    "            idx += 1  # skip ','\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return idx, (order_type, val_units)\n",
    "\n",
    "\n",
    "def parse_having(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'having':\n",
    "        return idx, []\n",
    "\n",
    "    idx += 1\n",
    "    idx, conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "def parse_limit(toks, start_idx):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    if idx < len_ and toks[idx] == 'limit':\n",
    "        idx += 2\n",
    "        return idx, int(toks[idx-1])\n",
    "\n",
    "    return idx, None\n",
    "\n",
    "\n",
    "def parse_sql(toks, start_idx, tables_with_alias, schema):\n",
    "    isBlock = False # indicate whether this is a block of sql/sub-sql\n",
    "    len_ = len(toks)\n",
    "    idx = start_idx\n",
    "\n",
    "    sql = {}\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    # parse from clause in order to get default tables\n",
    "    from_end_idx, table_units, conds, default_tables = parse_from(toks, start_idx, tables_with_alias, schema)\n",
    "    sql['from'] = {'table_units': table_units, 'conds': conds}\n",
    "    # select clause\n",
    "    _, select_col_units = parse_select(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    idx = from_end_idx\n",
    "    sql['select'] = select_col_units\n",
    "    # where clause\n",
    "    idx, where_conds = parse_where(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['where'] = where_conds\n",
    "    # group by clause\n",
    "    idx, group_col_units = parse_group_by(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['groupBy'] = group_col_units\n",
    "    # having clause\n",
    "    idx, having_conds = parse_having(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['having'] = having_conds\n",
    "    # order by clause\n",
    "    idx, order_col_units = parse_order_by(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['orderBy'] = order_col_units\n",
    "    # limit clause\n",
    "    idx, limit_val = parse_limit(toks, idx)\n",
    "    sql['limit'] = limit_val\n",
    "\n",
    "    idx = skip_semicolon(toks, idx)\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1  # skip ')'\n",
    "    idx = skip_semicolon(toks, idx)\n",
    "\n",
    "    # intersect/union/except clause\n",
    "    for op in SQL_OPS:  # initialize IUE\n",
    "        sql[op] = None\n",
    "    if idx < len_ and toks[idx] in SQL_OPS:\n",
    "        sql_op = toks[idx]\n",
    "        idx += 1\n",
    "        idx, IUE_sql = parse_sql(toks, idx, tables_with_alias, schema)\n",
    "        sql[sql_op] = IUE_sql\n",
    "    return idx, sql\n",
    "\n",
    "\n",
    "def load_data(fpath):\n",
    "    with open(fpath) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_sql(schema, query):\n",
    "    toks = tokenize(query)\n",
    "    tables_with_alias = get_tables_with_alias(schema.schema, toks)\n",
    "    _, sql = parse_sql(toks, 0, tables_with_alias, schema)\n",
    "\n",
    "    return sql\n",
    "\n",
    "\n",
    "def skip_semicolon(toks, start_idx):\n",
    "    idx = start_idx\n",
    "    while idx < len(toks) and toks[idx] == \";\":\n",
    "        idx += 1\n",
    "    return idx\n",
    "\n",
    "################################################## evalution.py 추출 함수 ##############################################\n",
    "\n",
    "\n",
    "\n",
    "# Flag to disable value evaluation\n",
    "DISABLE_VALUE = True\n",
    "# Flag to disable distinct in select evaluation\n",
    "DISABLE_DISTINCT = True\n",
    "\n",
    "\n",
    "HARDNESS = {\n",
    "    \"component1\": ('where', 'group', 'order', 'limit', 'join', 'or', 'like'),\n",
    "    \"component2\": ('except', 'union', 'intersect')\n",
    "}\n",
    "\n",
    "def get_nestedSQL(sql):\n",
    "    nested = []\n",
    "    for cond_unit in sql['from']['conds'][::2] + sql['where'][::2] + sql['having'][::2]:\n",
    "        if type(cond_unit[3]) is dict:\n",
    "            nested.append(cond_unit[3])\n",
    "        if type(cond_unit[4]) is dict:\n",
    "            nested.append(cond_unit[4])\n",
    "    if sql['intersect'] is not None:\n",
    "        nested.append(sql['intersect'])\n",
    "    if sql['except'] is not None:\n",
    "        nested.append(sql['except'])\n",
    "    if sql['union'] is not None:\n",
    "        nested.append(sql['union'])\n",
    "    return nested\n",
    "\n",
    "def get_keywords(sql):\n",
    "    res = set() # 중복점수를 허용하지 않는다. where ,groupby, having이든 하나만 인정\n",
    "    if len(sql['where']) > 0:\n",
    "        res.add('where')\n",
    "    if len(sql['groupBy']) > 0:\n",
    "        res.add('group')\n",
    "    if len(sql['having']) > 0:\n",
    "        res.add('having')\n",
    "    if len(sql['orderBy']) > 0:\n",
    "        res.add(sql['orderBy'][0])\n",
    "        res.add('order')\n",
    "    if sql['limit'] is not None:\n",
    "        res.add('limit')\n",
    "    if sql['except'] is not None:\n",
    "        res.add('except')\n",
    "    if sql['union'] is not None:\n",
    "        res.add('union')\n",
    "    if sql['intersect'] is not None:\n",
    "        res.add('intersect')\n",
    "\n",
    "    # or keyword\n",
    "    ao = sql['from']['conds'][1::2] + sql['where'][1::2] + sql['having'][1::2]\n",
    "    if len([token for token in ao if token == 'or']) > 0:\n",
    "        res.add('or')\n",
    "\n",
    "    cond_units = sql['from']['conds'][::2] + sql['where'][::2] + sql['having'][::2]\n",
    "    # not keyword\n",
    "    if len([cond_unit for cond_unit in cond_units if cond_unit[0]]) > 0:\n",
    "        res.add('not')\n",
    "\n",
    "    # in keyword\n",
    "    if len([cond_unit for cond_unit in cond_units if cond_unit[1] == WHERE_OPS.index('in')]) > 0:\n",
    "        res.add('in')\n",
    "\n",
    "    # like keyword\n",
    "    if len([cond_unit for cond_unit in cond_units if cond_unit[1] == WHERE_OPS.index('like')]) > 0:\n",
    "        res.add('like')\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def count_agg(units):\n",
    "    return len([unit for unit in units if has_agg(unit)])\n",
    "\n",
    "\n",
    "\n",
    "def has_agg(unit):\n",
    "    return unit[0] != AGG_OPS.index('none')\n",
    "\n",
    "\n",
    "def count_component1(sql):\n",
    "    count = 0\n",
    "    if len(sql['where']) > 0:\n",
    "        count += 1\n",
    "    if len(sql['groupBy']) > 0:\n",
    "        count += 1\n",
    "    if len(sql['orderBy']) > 0:\n",
    "        count += 1\n",
    "    if sql['limit'] is not None:\n",
    "        count += 1\n",
    "    if len(sql['from']['table_units']) > 0:  # JOIN 1개는 0점 ... 2개는 1점.\n",
    "        count += len(sql['from']['table_units']) - 1\n",
    "\n",
    "    ao = sql['from']['conds'][1::2] + sql['where'][1::2] + sql['having'][1::2]\n",
    "    count += len([token for token in ao if token == 'or'])\n",
    "    cond_units = sql['from']['conds'][::2] + sql['where'][::2] + sql['having'][::2]\n",
    "    count += len([cond_unit for cond_unit in cond_units if cond_unit[1] == WHERE_OPS.index('like')])\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_component2(sql):\n",
    "    nested = get_nestedSQL(sql)\n",
    "    return len(nested)\n",
    "\n",
    "\n",
    "def count_others(sql):\n",
    "    count = 0\n",
    "    # number of aggregation\n",
    "    agg_count = count_agg(sql['select'][1])\n",
    "    agg_count += count_agg(sql['where'][::2])\n",
    "    agg_count += count_agg(sql['groupBy'])\n",
    "    if len(sql['orderBy']) > 0:\n",
    "        agg_count += count_agg([unit[1] for unit in sql['orderBy'][1] if unit[1]] +\n",
    "                            [unit[2] for unit in sql['orderBy'][1] if unit[2]])\n",
    "    agg_count += count_agg(sql['having'])\n",
    "    if agg_count > 1:\n",
    "        count += 1\n",
    "\n",
    "    # number of select columns\n",
    "    if len(sql['select'][1]) > 1:\n",
    "        count += 1\n",
    "\n",
    "    # number of where conditions\n",
    "    if len(sql['where']) > 1:\n",
    "        count += 1\n",
    "\n",
    "    # number of group by clauses\n",
    "    if len(sql['groupBy']) > 1:\n",
    "        count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def eval_hardness(sql):\n",
    "    count_comp1_ = count_component1(sql)\n",
    "    count_comp2_ = count_component2(sql)\n",
    "    count_others_ = count_others(sql)\n",
    "\n",
    "    if count_comp1_ <= 1 and count_others_ == 0 and count_comp2_ == 0:\n",
    "        return \"easy\"\n",
    "    elif (count_others_ <= 2 and count_comp1_ <= 1 and count_comp2_ == 0) or (count_comp1_ <= 2 and count_others_ < 2 and count_comp2_ == 0):\n",
    "        return \"medium\"\n",
    "    elif (count_others_ > 2 and count_comp1_ <= 2 and count_comp2_ == 0) or (2 < count_comp1_ <= 3 and count_others_ <= 2 and count_comp2_ == 0) or (count_comp1_ <= 1 and count_others_ == 0 and count_comp2_ <= 1):\n",
    "        return \"hard\"\n",
    "    else:\n",
    "        return \"extra\"\n",
    "\n",
    "\n",
    "class Schema:\n",
    "    \"\"\"\n",
    "    Simple schema which maps table&column to a unique identifier\n",
    "    \"\"\"\n",
    "    def __init__(self, schema, table):\n",
    "        self._schema = schema\n",
    "        self._table = table\n",
    "        self._idMap = self._map(self._schema, self._table)\n",
    "\n",
    "    @property\n",
    "    def schema(self):\n",
    "        return self._schema\n",
    "\n",
    "    @property\n",
    "    def idMap(self):\n",
    "        return self._idMap\n",
    "\n",
    "    def _map(self, schema, table):\n",
    "        column_names_original = table['column_names_original']\n",
    "        table_names_original = table['table_names_original']\n",
    "        #print('column_names_original: ', column_names_original)\n",
    "        #print('table_names_original: ', table_names_original)\n",
    "        for i, (tab_id, col) in enumerate(column_names_original):\n",
    "            if tab_id == -1:\n",
    "                idMap = {'*': i}\n",
    "            else:\n",
    "                key = table_names_original[tab_id].lower()\n",
    "                val = col.lower()\n",
    "                idMap[key + \".\" + val] = i\n",
    "\n",
    "        for i, tab in enumerate(table_names_original):\n",
    "            key = tab.lower()\n",
    "            idMap[key] = i\n",
    "\n",
    "\n",
    "\n",
    "        return idMap\n",
    "\n",
    "def get_schemas_from_json(fpath):\n",
    "    with open(fpath) as f:\n",
    "        data = json.load(f)\n",
    "    db_names = [db['db_id'] for db in data]\n",
    "\n",
    "    tables = {}\n",
    "    schemas = {} # 전체 형태\n",
    "    for db in data:\n",
    "        db_id = db['db_id']\n",
    "        schema = {} #{'table': [col.lower, ..., ]} * -> __all__\n",
    "        column_names_original = db['column_names_original'] # 컬럼 갯수\n",
    "        table_names_original = db['table_names_original'] # 테이블명\n",
    "        tables[db_id] = {'column_names_original': column_names_original, 'table_names_original': table_names_original} # db_id에 해당하는 컬럼과 테이블명\n",
    "        for i, tabn in enumerate(table_names_original):\n",
    "            table = str(tabn.lower())\n",
    "            cols = [str(col.lower()) for td, col in column_names_original if td == i]\n",
    "            schema[table] = cols\n",
    "        schemas[db_id] = schema\n",
    "\n",
    "    return schemas, db_names, tables\n",
    "\n",
    "############################################################################################################################################################\n",
    "\n",
    "def sql_level(query, db_id) : # db정보를 json으로 가져와서 비교하는 함수\n",
    "    sql = query\n",
    "    db_id = db_id\n",
    "    table_file = \"C:/Users/yoonsub/test05.json\" # cp949 에러,메모장에서 anis다시 인코딩시 한글은 사라짐\n",
    "\n",
    "    schemas, db_names, tables = get_schemas_from_json(table_file)\n",
    "    schema = schemas[db_id]\n",
    "    table = tables[db_id]\n",
    "    print(\"테이블 : \" , table)\n",
    "\n",
    "    schema = Schema(schema, table)\n",
    "    sql_label = get_sql(schema, sql)\n",
    "\n",
    "    comp_1 = count_component1(sql_label)\n",
    "    comp_2 = count_component2(sql_label)\n",
    "    comp_other = count_others(sql_label)\n",
    "\n",
    "    level = eval_hardness(sql_label)\n",
    "    score = f\"{comp_1}-{comp_2}-{comp_other}\"\n",
    "\n",
    "    print(\"쿼리문: \",sql)\n",
    "    print(f\"유형 : {score} , 난이도 :{level}\")\n",
    "\n",
    "\n",
    "def spider_level_query(query) : # db에 접속해서 쿼리문만으로 테이블명과 db_id를 가져와서 쿼리 난이도를 판별하는 함수\n",
    "    # 티베로 db 접속\n",
    "    conn = jaydebeapi.connect(\n",
    "        \"com.tmax.tibero.jdbc.TbDriver\",\n",
    "        \"jdbc:tibero:thin:@172.7.0.23:8629:tibero\",\n",
    "        [\"labelon\", \"euclid!@)$labelon\"],\n",
    "        \"tibero6-jdbc.jar\",\n",
    "        )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # 쿼리를 공백 분리 후 FROM 인덱스로 테이블명 확인\n",
    "    query = query.upper()\n",
    "    query = re.sub(' +', ' ', query)  # 다중공백을 변환\n",
    "    query_list = query.split(\" \")\n",
    "    table_index = query_list.index(\"FROM\") + 1\n",
    "    table_name = str(query_list[table_index])\n",
    "\n",
    "    # 확인한 테이블명으로 data_sql 쿼리문 실행하여 db_id, physical_data_id등을 조회\n",
    "    data_sql = f\"SELECT PHYSICAL_TABLE_NAME,id,DATA_BASIC_ID FROM MANAGE_PHYSICAL_TABLE WHERE LOGICAL_TABLE_ENGLISH = '{table_name}'\"\n",
    "    #print(data_sql)\n",
    "    cur.execute(data_sql)\n",
    "    data01 = cur.fetchall()\n",
    "\n",
    "    db_id = data01[0][1]\n",
    "    print(\"db_id : \", db_id)\n",
    "\n",
    "    column_sql = f\"SELECT LOGICAL_COLUMN_ENGLISH FROM MANAGE_PHYSICAL_COLUMN WHERE DATA_PHYSICAL_ID = '{db_id}'\" # physical_table의 id를 컬럼에서 physical_data_id로 조회 (pk -> fk )\n",
    "    cur.execute(column_sql)\n",
    "    column_list = cur.fetchall() # pyysical_data_id로  해당 컬럼 모두 조회\n",
    "\n",
    "    # 분석용 sql구문 전처리\n",
    "    cols = []\n",
    "    for i in column_list :\n",
    "        i = str(i).replace(\"(\" , \"\").replace(\")\" , \"\").replace(\",\" , \"\").replace(\"'\" , \"\")\n",
    "        i = i.lower()\n",
    "        cols.append(i)\n",
    "    #print(cols)\n",
    "\n",
    "    #column_names_original create\n",
    "    column_names_original = []\n",
    "    for col in column_list:\n",
    "        col = str(col).replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "        col = col.upper()\n",
    "        for j in [0]:\n",
    "            add_col = [j, col]\n",
    "            column_names_original.append(add_col)\n",
    "    #print(column_names_original)\n",
    "\n",
    "    first_col = [[-1, '*']]\n",
    "    column_names_original = first_col + column_names_original\n",
    "    table_names_original = [table_name]\n",
    "\n",
    "    # schemas 형태\n",
    "    schemas = {db_id : {table_name.lower() : cols}} # <- table with alie 대문자,소문자 차이는 여기서 보정\n",
    "    tables = { db_id  : { \"column_names_original\" : column_names_original, \"table_names_original\" : table_names_original}}\n",
    "    #print(tables)\n",
    "\n",
    "    ##### level 판별구간 ####\n",
    "    sql = query\n",
    "    schema = schemas[db_id]\n",
    "    table = tables[db_id]\n",
    "\n",
    "\n",
    "    schema = Schema(schema, table)  # class Schema 를 사용 오리지널 컬럼과 , 오리지널 테이블을 조건을 바꿔야한다.\n",
    "    sql_label = get_sql(schema, sql)\n",
    "\n",
    "\n",
    "    comp_1 = count_component1(sql_label)\n",
    "    comp_2 = count_component2(sql_label)\n",
    "    comp_other = count_others(sql_label)\n",
    "\n",
    "    level = eval_hardness(sql_label)\n",
    "    score = f\"{comp_1}-{comp_2}-{comp_other}\"\n",
    "\n",
    "    print(\"query : \", sql)\n",
    "    print(f\"type : {score} , level : {level}\")\n",
    "\n",
    "\n",
    "spider_level_query(\"SELECT year, creASe_or_decreASe_overall_industrial_exports FROM IMPORT_EXPORT_STS_MOTIE_TEXTILE_I WHERE year IN (SELECT year FROM IMPORT_EXPORT_STS_MOTIE_TEXTILE_I WHERE tal_industrial_income_amount > 100000)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f659a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
